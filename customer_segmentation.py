# -*- coding: utf-8 -*-
"""Customer Segmentation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/customer-segmentation-f00358bd-e5b9-4fb6-9f55-deb92d1b8790.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20241020/auto/storage/goog4_request%26X-Goog-Date%3D20241020T101151Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D2233dfdc7e0b855a2a42c0a34863e6e978906dcc77732119b0d2e4caf5ca1e65ec9af0e0873296ef91af66d6df61bb62d23ce316ad23100d5e7bdb12843ab8d352fabfd1775bb6319cf6bc7dcd9467de54297a4570cdc6dac03b81cd50c712b7f3544e314844d76d9bf31eb987f593d580a09d2020d500904707e6c62b6dfdbeff656d9564335a23e72e52370e3083c2edda8423365ad2eeff1155ec6f2f054327748c95703089fcdc28f3ee3fdbbf6dde637894a823097382993a81733b5be49ec29fe3cbc950d67d7eec82211077965700a747508ac8213120115ef81e7efc21005930266e701b4fd3d6edb34110d264bb7e6bf42795750854d2c99aed43ea
"""

import pandas as pd
import numpy as np
import seaborn as sns
import importlib
import datetime as dt
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.cluster import KMeans



pd.set_option('display.max_columns', None)
pd.set_option('display.float_format', lambda x: '%.3f' % x)

import warnings
warnings.filterwarnings("ignore")
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.simplefilter(action='ignore', category=DeprecationWarning)

file_path = "/kaggle/input/online-retail-ii-data-set-from-ml-repository/Year 2010-2011.csv"


try:
    df = pd.read_csv(file_path, encoding='utf-8')
except UnicodeDecodeError:
    df = pd.read_csv(file_path, encoding='ISO-8859-1')

# Veri setini gÃ¶ster
df.head()

"""## Exploratory Data Analysis (EDA)"""

def describe_func(df):
    print("******************* Info *******************\n")
    df.info()
    print("\n")

    print("******************* Shape*******************\n")
    print(df.shape)
    print("\n")

    print("******************* Head *******************\n")
    print(df.head())
    print("\n")

    print("*******************Tail *******************\n")
    print(df.tail())
    print("\n")

    print("******************* NA *******************\n")
    print(df.isnull().sum())
    print("\n")

    print("******************* Describe *******************\n")
    print(df.describe().T)
    print("\n")

describe_func(df)

# Drop NA Values

df.dropna(inplace=True)
df.isnull().sum()

# Removing cancellations starting with C

df_Invoice = pd.DataFrame({"Invoice":[row for row in df["Invoice"].values if "C" not in str(row)]})

df_Invoice = df_Invoice.drop_duplicates("Invoice")

df = df.merge(df_Invoice, on = "Invoice")

df.shape

# Let's clear 0 or negative values in the Price and Quantity columns

df = df[(df["Quantity"] > 0) & (df["Price"] > 0)]

# Unique Number of Products(Description)

df.Description.nunique()

# Unique Number of Products(StockCode)

df.StockCode.nunique()

# It seems that the unique values are not consistent for each Stock Code with their explanations. Each Stock Code should have unique Description values. Let's correct this.

# In the first step, let's remove more than one product from the stockCode number

df_product = df[["Description", "StockCode"]].drop_duplicates()
stockcode_counts = df.groupby("StockCode")["Description"].nunique()

df_product = stockcode_counts[stockcode_counts > 1].index.to_list()


df = df[~df["StockCode"].isin(df_product)]

# In the second step, let's remove more than one product from the Description number.

df_product = df[["Description", "StockCode"]].drop_duplicates()
Description_counts = df.groupby("Description")["StockCode"].nunique()

df_product = Description_counts[Description_counts > 1].index.tolist()

df = df[~df["Description"].isin(df_product)]

print(df.StockCode.nunique())
print(df.Description.nunique())

# Let's calculate the total sales value per invoice

df["TotalPrice"] = df["Quantity"] * df["Price"]

df.head()

# Let's convert the InvoiceDate column to datetime format

df["InvoiceDate"] = pd.to_datetime(df["InvoiceDate"])

snapshot_date = df['InvoiceDate'].max() + dt.timedelta(days=1)

rfm = df.groupby('Customer ID').agg({
    'InvoiceDate': lambda x: (snapshot_date - x.max()).days,
    'Invoice': "nunique",
    'Price': "sum"
})
rfm.columns = ['Recency', 'Frequency', 'Monetary']

rfm.head()

# KMeans
kmeans = KMeans(n_clusters=4, random_state=42)
rfm['Cluster'] = kmeans.fit_predict(rfm)

# Count the number of customers in each segment
segment_counts = rfm['Cluster'].value_counts()

plt.figure(figsize=(10, 7))
colors = sns.color_palette('pastel', len(segment_counts))
plt.pie(segment_counts, labels=segment_counts.index, autopct='%1.1f%%', startangle=140, colors=colors)
plt.title('Customer Segment Distribution')
plt.show()


# Display segment characteristics summary
cluster_summary = rfm.groupby('Cluster').agg({
    'Recency': 'mean',
    'Frequency': 'mean',
    'Monetary': 'mean'
}).reset_index()

cluster_summary

X = df[["Quantity", "Price"]]
y = df["TotalPrice"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

plt.scatter(y_test, y_pred)
plt.xlabel('Real Sales')
plt.ylabel('Predict Sales')
plt.title('Sales Forecast')
plt.show()

print("Model Coef:", model.coef_)

# Extract features from InvoiceDate
df['Year'] = df['InvoiceDate'].dt.year
df['Month'] = df['InvoiceDate'].dt.month
df['Day'] = df['InvoiceDate'].dt.day
df['Hour'] = df['InvoiceDate'].dt.hour
df['Minute'] = df['InvoiceDate'].dt.minute

# Prepare the feature set and target variable
features = ['Quantity', 'Price', 'Year', 'Month', 'Day', 'Hour', 'Minute']
X = df[features]
y = df['TotalPrice']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Initialize the model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model
rf_model.fit(X_train, y_train)
# Make predictions on the test set
y_pred_rf = rf_model.predict(X_test)

# Evaluate the model
mse_rf = mean_squared_error(y_test, y_pred_rf)
print(f'Mean Squared Error: {mse_rf}')

# Plot actual vs predicted values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_rf, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Actual vs Predicted Sales (Random Forest)')
plt.show()

importances = rf_model.feature_importances_
feature_names = X.columns

# Create a DataFrame for better visualization
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
})

# Sort the DataFrame by importance
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Visualization
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df, palette='viridis')
plt.title('Feature Importances in Random Forest Model')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

